{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install praw\n",
    "# %pip install google-generativeai\n",
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0, # controls randomness. 0 = most deterministic (always selects highest probability token).\n",
    "  \"top_p\": 0, # nucleus sampling: limits token selection to the most probable. 0 = most deterministic (used when temperature > 0).\n",
    "  \"top_k\": 1, # restricts to top 'k' tokens. 1 = most deterministic (used when temperature > 0).\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-exp-1206\",\n",
    "  generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Subscribers</th>\n",
       "      <th>Description</th>\n",
       "      <th>Over 18</th>\n",
       "      <th>Submission Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>65915170</td>\n",
       "      <td>Reddit's largest humor depository</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>50406011</td>\n",
       "      <td>r/AskReddit is the place to ask and answer tho...</td>\n",
       "      <td>False</td>\n",
       "      <td>self</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaming</td>\n",
       "      <td>44958493</td>\n",
       "      <td>The Number One Gaming forum on the Internet.</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>43570215</td>\n",
       "      <td>A place for major news from around the world, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todayilearned</td>\n",
       "      <td>39208843</td>\n",
       "      <td>You learn something new every day; what did yo...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aww</td>\n",
       "      <td>37359700</td>\n",
       "      <td>Things that make you go AWW! -- like puppies, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Music</td>\n",
       "      <td>35820412</td>\n",
       "      <td>Reddit’s #1 Music Community</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>memes</td>\n",
       "      <td>35219610</td>\n",
       "      <td>Memes!\\n\\nA way of describing cultural informa...</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>movies</td>\n",
       "      <td>34170973</td>\n",
       "      <td>The goal of /r/Movies is to provide an inclusi...</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>33646166</td>\n",
       "      <td>A subreddit for sharing those miniature epipha...</td>\n",
       "      <td>False</td>\n",
       "      <td>self</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name  Subscribers  \\\n",
       "0           funny     65915170   \n",
       "1       AskReddit     50406011   \n",
       "2          gaming     44958493   \n",
       "3       worldnews     43570215   \n",
       "4   todayilearned     39208843   \n",
       "5             aww     37359700   \n",
       "6           Music     35820412   \n",
       "7           memes     35219610   \n",
       "8          movies     34170973   \n",
       "9  Showerthoughts     33646166   \n",
       "\n",
       "                                         Description  Over 18 Submission Type  \n",
       "0                  Reddit's largest humor depository    False             any  \n",
       "1  r/AskReddit is the place to ask and answer tho...    False            self  \n",
       "2       The Number One Gaming forum on the Internet.    False             any  \n",
       "3  A place for major news from around the world, ...    False            link  \n",
       "4  You learn something new every day; what did yo...    False            link  \n",
       "5  Things that make you go AWW! -- like puppies, ...    False            link  \n",
       "6                        Reddit’s #1 Music Community    False             any  \n",
       "7  Memes!\\n\\nA way of describing cultural informa...    False             any  \n",
       "8  The goal of /r/Movies is to provide an inclusi...    False             any  \n",
       "9  A subreddit for sharing those miniature epipha...    False            self  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ[\"PRAW_CLIENT_ID\"],\n",
    "    client_secret=os.environ[\"PRAW_CLIENT_SECRET\"],\n",
    "    user_agent=os.environ[\"PRAW_USER_AGENT\"],\n",
    "    username=os.environ[\"PRAW_USERNAME\"],\n",
    "    password=os.environ[\"PRAW_PASSWORD\"],\n",
    ")\n",
    "\n",
    "# Fetch a large subset of popular subreddits (large limit makes this representative of the largest overall subreddits by subscribers, check: https://gummysearch.com/tools/top-subreddits/)\n",
    "subreddits = list(reddit.subreddits.popular(limit=1000))\n",
    "\n",
    "# Create a DataFrame using list comprehension for better performance\n",
    "subs_df = pd.DataFrame([{\n",
    "    \"Name\": subreddit.display_name,\n",
    "    \"Subscribers\": subreddit.subscribers,\n",
    "    \"Description\": subreddit.public_description,\n",
    "    \"Over 18\": subreddit.over18,\n",
    "    \"Submission Type\": subreddit.submission_type\n",
    "} for subreddit in subreddits]).sort_values(by=\"Subscribers\", ascending=False, ignore_index=True)\n",
    "\n",
    "# Print the top 10\n",
    "subs_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news politics discussion war vaccines\n",
      "controversial conflict debate opinion world\n",
      "current events election government policy social issues\n",
      "conspiracy exposed truth censorship freedom\n",
      "rights activism protest revolution change\n",
      "reform corruption scandal crime justice\n",
      "law police military security surveillance\n",
      "privacy technology media propaganda bias\n",
      "fake news disinformation misinformation manipulation control\n",
      "power ideology culture society\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "chat_session = model.start_chat()\n",
    "\n",
    "response = chat_session.send_message(\"What are some keywords I can use to create a list of subreddits which are likely to be influenced by bots because of their controversial nature? These are keywords that I would look for within a subreddit's name or description. For example: \\\"news\\\", \\\"politics\\\", \\\"discussion\\\", \\\"war\\\", \\\"vaccines\\\", \\\"controversial\\\", \\\"conflict\\\", etc.\\n\\nKeep the answer short, only including 50 keywords and saving them in a python list as follows [\\\"key1\\\",\\\"key2\\\",...]. Send the output as text not as code.\")\n",
    "\n",
    "bot_influence_keywords = ast.literal_eval(response.candidates[0].content.parts[0].text.replace(\"\\n\", \"\"))\n",
    "\n",
    "for i in range(0, len(bot_influence_keywords), 5):\n",
    "    print(*bot_influence_keywords[i:i+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Subscribers</th>\n",
       "      <th>Submission Type</th>\n",
       "      <th>Bot Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>43570215</td>\n",
       "      <td>link</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>29224905</td>\n",
       "      <td>link</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "      <td>17726644</td>\n",
       "      <td>any</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>science</td>\n",
       "      <td>33423652</td>\n",
       "      <td>link</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>9372102</td>\n",
       "      <td>any</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>8695025</td>\n",
       "      <td>link</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>movies</td>\n",
       "      <td>34170973</td>\n",
       "      <td>any</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>askscience</td>\n",
       "      <td>25977825</td>\n",
       "      <td>self</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>books</td>\n",
       "      <td>25660597</td>\n",
       "      <td>any</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>23088653</td>\n",
       "      <td>self</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name  Subscribers Submission Type  Bot Score\n",
       "0       worldnews     43570215            link          9\n",
       "1            news     29224905            link          9\n",
       "2      technology     17726644             any          9\n",
       "3         science     33423652            link          8\n",
       "4  CryptoCurrency      9372102             any          8\n",
       "5        politics      8695025            link          8\n",
       "6          movies     34170973             any          7\n",
       "7      askscience     25977825            self          7\n",
       "8           books     25660597             any          7\n",
       "9   AmItheAsshole     23088653            self          7"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score subreddits based on subscribers and keywords in description\n",
    "def calculate_bot_influence_score(row):\n",
    "    score = 0\n",
    "    \n",
    "    # Large subscriber base increases potential for bot activity\n",
    "    if row['Subscribers'] > 10000000:\n",
    "        score += 5\n",
    "    elif row['Subscribers'] > 5000000:\n",
    "        score += 4\n",
    "    elif row['Subscribers'] > 1000000:\n",
    "        score += 3\n",
    "        \n",
    "    # Check for keywords in description and subreddit name\n",
    "    description = row['Description'].lower()\n",
    "    sub_name = row['Name'].lower()\n",
    "    for keyword in bot_influence_keywords:\n",
    "        if keyword in description:\n",
    "            score += 1\n",
    "        if keyword in sub_name:\n",
    "            score += 1\n",
    "            \n",
    "    return score\n",
    "\n",
    "subs_df['Bot Score'] = subs_df.apply(calculate_bot_influence_score, axis=1)\n",
    "\n",
    "# Get top 10 most vulnerable subreddits\n",
    "top_vulnerable = subs_df.nlargest(10, 'Bot Score')[['Name', 'Subscribers', 'Submission Type', 'Bot Score']].reset_index(drop=True)\n",
    "top_vulnerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from r/worldnews...\n",
      "Fetching data from r/news...\n",
      "Fetching data from r/AmItheAsshole...\n",
      "Fetching data from r/Art...\n",
      "Fetching data from r/sports...\n",
      "Fetching data from r/personalfinance...\n",
      "Fetching data from r/UpliftingNews...\n",
      "Fetching data from r/politics...\n",
      "Fetching data from r/stocks...\n",
      "Fetching data from r/AskReddit...\n",
      "Data collection complete!\n"
     ]
    }
   ],
   "source": [
    "# 5. Function to Fetch Posts and Comments\n",
    "def fetch_posts_and_comments(subreddit_name, num_posts=100, num_comments=100):\n",
    "    \"\"\"\n",
    "    Fetches posts and their top-level comments from a subreddit.\n",
    "\n",
    "    Args:\n",
    "        subreddit_name: The name of the subreddit.\n",
    "        num_posts: The maximum number of posts to fetch.\n",
    "        num_comments: The maximum number of top-level comments to fetch per post.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a post and its comments.\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    try:\n",
    "        for post in subreddit.new(limit=num_posts):  # You can change 'hot' to 'new', 'rising', etc.\n",
    "            post_data = {\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"post_id\": post.id,\n",
    "                \"post_title\": post.title,\n",
    "                \"post_author\": str(post.author),\n",
    "                \"post_score\": post.score,\n",
    "                \"post_upvote_ratio\": post.upvote_ratio,\n",
    "                \"post_url\": post.url,\n",
    "                \"post_selftext\": post.selftext,\n",
    "                \"post_created_utc\": post.created_utc,\n",
    "                \"comments\": []\n",
    "            }\n",
    "\n",
    "            post.comments.replace_more(limit=0)  # Fetch only top-level comments, ignore \"more comments\"\n",
    "            \n",
    "            comment_count = 0\n",
    "            for comment in post.comments:\n",
    "                if comment_count >= num_comments:\n",
    "                    break\n",
    "                post_data[\"comments\"].append({\n",
    "                    \"comment_id\": comment.id,\n",
    "                    \"comment_author\": str(comment.author),\n",
    "                    \"comment_body\": comment.body,\n",
    "                    \"comment_score\": comment.score,\n",
    "                    \"comment_created_utc\": comment.created_utc\n",
    "                })\n",
    "                comment_count += 1\n",
    "\n",
    "            posts_data.append(post_data)\n",
    "            \n",
    "            # Respect API rate limits\n",
    "            # time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from r/{subreddit_name}: {e}\")\n",
    "\n",
    "    return posts_data\n",
    "\n",
    "# 6. Main Data Collection Loop\n",
    "all_data = []\n",
    "for subreddit_name in top_vulnerable['Name']:\n",
    "    print(f\"Fetching data from r/{subreddit_name}...\")\n",
    "    subreddit_data = fetch_posts_and_comments(subreddit_name, num_posts=50, num_comments=50)  # Adjust numbers as needed\n",
    "    all_data.extend(subreddit_data)\n",
    "\n",
    "# 7. Convert to DataFrame\n",
    "reddit_data_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Convert lists of comments to a separate DataFrame if desired\n",
    "comments_data = []\n",
    "for index, row in reddit_data_df.iterrows():\n",
    "    for comment in row['comments']:\n",
    "        comment['post_id'] = row['post_id'] # add the relationship\n",
    "        comments_data.append(comment)\n",
    "comments_df = pd.DataFrame(comments_data)\n",
    "# Expand the comments into its own columns\n",
    "reddit_data_df = pd.concat([reddit_data_df.drop(['comments'], axis=1), pd.DataFrame(reddit_data_df['comments'].tolist()).add_prefix('comment_')], axis=1)\n",
    "\n",
    "# 8. Save to CSV (or other format)\n",
    "reddit_data_df.to_csv(\"reddit_posts_and_comments.csv\", index=False)\n",
    "comments_df.to_csv(\"comments.csv\", index=False)\n",
    "\n",
    "print(\"Data collection complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
