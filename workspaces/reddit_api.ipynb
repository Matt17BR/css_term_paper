{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install praw\n",
    "# %pip install google-generativeai\n",
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0, # controls randomness. 0 = most deterministic (always selects highest probability token).\n",
    "  \"top_p\": 0, # nucleus sampling: limits token selection to the most probable. 0 = most deterministic (used when temperature > 0).\n",
    "  \"top_k\": 1, # restricts to top 'k' tokens. 1 = most deterministic (used when temperature > 0).\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-exp-1206\",\n",
    "  generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Subscribers</th>\n",
       "      <th>Description</th>\n",
       "      <th>Over 18</th>\n",
       "      <th>Submission Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>65908865</td>\n",
       "      <td>Reddit's largest humor depository</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>50393197</td>\n",
       "      <td>r/AskReddit is the place to ask and answer tho...</td>\n",
       "      <td>False</td>\n",
       "      <td>self</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaming</td>\n",
       "      <td>44952145</td>\n",
       "      <td>The Number One Gaming forum on the Internet.</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>43561323</td>\n",
       "      <td>A place for major news from around the world, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todayilearned</td>\n",
       "      <td>39203260</td>\n",
       "      <td>You learn something new every day; what did yo...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aww</td>\n",
       "      <td>37357549</td>\n",
       "      <td>Things that make you go AWW! -- like puppies, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Music</td>\n",
       "      <td>35814017</td>\n",
       "      <td>Reddit’s #1 Music Community</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>memes</td>\n",
       "      <td>35216284</td>\n",
       "      <td>Memes!\\n\\nA way of describing cultural informa...</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>movies</td>\n",
       "      <td>34168562</td>\n",
       "      <td>The goal of /r/Movies is to provide an inclusi...</td>\n",
       "      <td>False</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>33641886</td>\n",
       "      <td>A subreddit for sharing those miniature epipha...</td>\n",
       "      <td>False</td>\n",
       "      <td>self</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name  Subscribers  \\\n",
       "0           funny     65908865   \n",
       "1       AskReddit     50393197   \n",
       "2          gaming     44952145   \n",
       "3       worldnews     43561323   \n",
       "4   todayilearned     39203260   \n",
       "5             aww     37357549   \n",
       "6           Music     35814017   \n",
       "7           memes     35216284   \n",
       "8          movies     34168562   \n",
       "9  Showerthoughts     33641886   \n",
       "\n",
       "                                         Description  Over 18 Submission Type  \n",
       "0                  Reddit's largest humor depository    False             any  \n",
       "1  r/AskReddit is the place to ask and answer tho...    False            self  \n",
       "2       The Number One Gaming forum on the Internet.    False             any  \n",
       "3  A place for major news from around the world, ...    False            link  \n",
       "4  You learn something new every day; what did yo...    False            link  \n",
       "5  Things that make you go AWW! -- like puppies, ...    False            link  \n",
       "6                        Reddit’s #1 Music Community    False             any  \n",
       "7  Memes!\\n\\nA way of describing cultural informa...    False             any  \n",
       "8  The goal of /r/Movies is to provide an inclusi...    False             any  \n",
       "9  A subreddit for sharing those miniature epipha...    False            self  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ[\"PRAW_CLIENT_ID\"],\n",
    "    client_secret=os.environ[\"PRAW_CLIENT_SECRET\"],\n",
    "    user_agent=os.environ[\"PRAW_USER_AGENT\"],\n",
    "    username=os.environ[\"PRAW_USERNAME\"],\n",
    "    password=os.environ[\"PRAW_PASSWORD\"],\n",
    ")\n",
    "\n",
    "# Fetch a large subset of popular subreddits (large limit makes this representative of the largest overall subreddits by subscribers, check: https://gummysearch.com/tools/top-subreddits/)\n",
    "subreddits = list(reddit.subreddits.popular(limit=1000))\n",
    "\n",
    "# Create a DataFrame using list comprehension for better performance\n",
    "subs_df = pd.DataFrame([{\n",
    "    \"Name\": subreddit.display_name,\n",
    "    \"Subscribers\": subreddit.subscribers,\n",
    "    \"Description\": subreddit.public_description,\n",
    "    \"Over 18\": subreddit.over18,\n",
    "    \"Submission Type\": subreddit.submission_type\n",
    "} for subreddit in subreddits]).sort_values(by=\"Subscribers\", ascending=False, ignore_index=True)\n",
    "\n",
    "# Print the top 10\n",
    "subs_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['politics', 'elections', 'news', 'war', 'conflict', 'vaccines', 'conspiracy', 'finance', 'crypto', 'stocks', 'technology', 'AI', 'socialmedia', 'propaganda', 'misinformation', 'disinformation', 'activism', 'protest', 'government', 'worldnews']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "chat_session = model.start_chat()\n",
    "\n",
    "response = chat_session.send_message(\"What are some keywords i can use to create a list of subreddits which are likely to be influenced by bots? For example: \\\"news\\\", \\\"politics\\\", \\\"war\\\", \\\"vaccines\\\", etc.\\n\\nKeep the answer short, only including about 20 keywords and saving them in a python list as follows [\\\"key1\\\",\\\"key2\\\",...]. Send the output as text not as code.\")\n",
    "\n",
    "bot_influence_keywords = ast.literal_eval(response.candidates[0].content.parts[0].text.replace(\"\\n\", \"\"))\n",
    "\n",
    "print(bot_influence_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Subscribers</th>\n",
       "      <th>Submission Type</th>\n",
       "      <th>Bot Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>43561323</td>\n",
       "      <td>link</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>news</td>\n",
       "      <td>29222258</td>\n",
       "      <td>link</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>23079120</td>\n",
       "      <td>self</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Art</td>\n",
       "      <td>22367671</td>\n",
       "      <td>link</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sports</td>\n",
       "      <td>21707364</td>\n",
       "      <td>link</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>personalfinance</td>\n",
       "      <td>20264014</td>\n",
       "      <td>self</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>UpliftingNews</td>\n",
       "      <td>20189482</td>\n",
       "      <td>link</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>politics</td>\n",
       "      <td>8694748</td>\n",
       "      <td>link</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>stocks</td>\n",
       "      <td>8290475</td>\n",
       "      <td>self</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>50393197</td>\n",
       "      <td>self</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name  Subscribers Submission Type  Bot Score\n",
       "3         worldnews     43561323            link          5\n",
       "13             news     29222258            link          4\n",
       "22    AmItheAsshole     23079120            self          4\n",
       "26              Art     22367671            link          4\n",
       "27           sports     21707364            link          4\n",
       "30  personalfinance     20264014            self          4\n",
       "31    UpliftingNews     20189482            link          4\n",
       "62         politics      8694748            link          4\n",
       "65           stocks      8290475            self          4\n",
       "1         AskReddit     50393197            self          3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score subreddits based on subscribers and keywords in description\n",
    "def calculate_bot_influence_score(row):\n",
    "    score = 0\n",
    "    \n",
    "    # Large subscriber base increases potential for bot activity\n",
    "    if row['Subscribers'] > 10000000:\n",
    "        score += 3\n",
    "    elif row['Subscribers'] > 5000000:\n",
    "        score += 2\n",
    "    elif row['Subscribers'] > 1000000:\n",
    "        score += 1\n",
    "        \n",
    "    # Check for keywords in description and subreddit name\n",
    "    description = row['Description'].lower()\n",
    "    sub_name = row['Name'].lower()\n",
    "    for keyword in bot_influence_keywords:\n",
    "        if keyword in description:\n",
    "            score += 1\n",
    "        elif keyword in sub_name:\n",
    "            score += 1\n",
    "            \n",
    "    return score\n",
    "\n",
    "# Filter for link/self submissions and calculate scores\n",
    "bot_vulnerable_subs = subs_df[\n",
    "    (subs_df['Submission Type'].isin(['link', 'self']))\n",
    "].copy()\n",
    "\n",
    "bot_vulnerable_subs['Bot Score'] = bot_vulnerable_subs.apply(calculate_bot_influence_score, axis=1)\n",
    "\n",
    "# Get top 10 most vulnerable subreddits\n",
    "top_vulnerable = bot_vulnerable_subs.nlargest(10, 'Bot Score')[['Name', 'Subscribers', 'Submission Type', 'Bot Score']]\n",
    "top_vulnerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from r/worldnews...\n",
      "Fetching data from r/news...\n",
      "Fetching data from r/AmItheAsshole...\n",
      "Fetching data from r/Art...\n",
      "Fetching data from r/sports...\n",
      "Fetching data from r/personalfinance...\n",
      "Fetching data from r/UpliftingNews...\n",
      "Fetching data from r/politics...\n",
      "Fetching data from r/stocks...\n",
      "Fetching data from r/AskReddit...\n",
      "Data collection complete!\n"
     ]
    }
   ],
   "source": [
    "# 5. Function to Fetch Posts and Comments\n",
    "def fetch_posts_and_comments(subreddit_name, num_posts=100, num_comments=50):\n",
    "    \"\"\"\n",
    "    Fetches posts and their top-level comments from a subreddit.\n",
    "\n",
    "    Args:\n",
    "        subreddit_name: The name of the subreddit.\n",
    "        num_posts: The maximum number of posts to fetch.\n",
    "        num_comments: The maximum number of top-level comments to fetch per post.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a post and its comments.\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    try:\n",
    "        for post in subreddit.hot(limit=num_posts):  # You can change 'hot' to 'new', 'rising', etc.\n",
    "            post_data = {\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"post_id\": post.id,\n",
    "                \"post_title\": post.title,\n",
    "                \"post_author\": str(post.author),\n",
    "                \"post_score\": post.score,\n",
    "                \"post_upvote_ratio\": post.upvote_ratio,\n",
    "                \"post_url\": post.url,\n",
    "                \"post_selftext\": post.selftext,\n",
    "                \"post_created_utc\": post.created_utc,\n",
    "                \"comments\": []\n",
    "            }\n",
    "\n",
    "            post.comments.replace_more(limit=0)  # Fetch only top-level comments, ignore \"more comments\"\n",
    "            \n",
    "            comment_count = 0\n",
    "            for comment in post.comments:\n",
    "                if comment_count >= num_comments:\n",
    "                    break\n",
    "                post_data[\"comments\"].append({\n",
    "                    \"comment_id\": comment.id,\n",
    "                    \"comment_author\": str(comment.author),\n",
    "                    \"comment_body\": comment.body,\n",
    "                    \"comment_score\": comment.score,\n",
    "                    \"comment_created_utc\": comment.created_utc\n",
    "                })\n",
    "                comment_count += 1\n",
    "\n",
    "            posts_data.append(post_data)\n",
    "            \n",
    "            # Respect API rate limits\n",
    "            # time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from r/{subreddit_name}: {e}\")\n",
    "\n",
    "    return posts_data\n",
    "\n",
    "# 6. Main Data Collection Loop\n",
    "all_data = []\n",
    "for subreddit_name in top_vulnerable['Name']:\n",
    "    print(f\"Fetching data from r/{subreddit_name}...\")\n",
    "    subreddit_data = fetch_posts_and_comments(subreddit_name, num_posts=50, num_comments=50)  # Adjust numbers as needed\n",
    "    all_data.extend(subreddit_data)\n",
    "\n",
    "# 7. Convert to DataFrame\n",
    "reddit_data_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Convert lists of comments to a separate DataFrame if desired\n",
    "comments_data = []\n",
    "for index, row in reddit_data_df.iterrows():\n",
    "    for comment in row['comments']:\n",
    "        comment['post_id'] = row['post_id'] # add the relationship\n",
    "        comments_data.append(comment)\n",
    "comments_df = pd.DataFrame(comments_data)\n",
    "# Expand the comments into its own columns\n",
    "reddit_data_df = pd.concat([reddit_data_df.drop(['comments'], axis=1), pd.DataFrame(reddit_data_df['comments'].tolist()).add_prefix('comment_')], axis=1)\n",
    "\n",
    "# 8. Save to CSV (or other format)\n",
    "reddit_data_df.to_csv(\"reddit_posts_and_comments.csv\", index=False)\n",
    "comments_df.to_csv(\"comments.csv\", index=False)\n",
    "\n",
    "print(\"Data collection complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
